//
// Copyright (c) 2016-2018 Contributors to the Eclipse Foundation
//
// See the NOTICE file(s) distributed with this work for additional
// information regarding copyright ownership.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

== Architecture


This chapter describes the architectural overview of how metrics are setup, stored and exposed for consumption.
This chapter also lists the various scopes of metrics.

See section <<required-metrics>> for more information regarding metrics that are required for each vendor.

See section <<app-metrics-api>> for more information regarding the application metrics programming model.

[[metrics-setup]]
=== Metrics Setup

Metrics that are exposed need to be configured in the server. On top of the pure metrics, metadata needs to be provided.

The following three sets of sub-resource (scopes) are exposed.

* base: metrics that all MicroProfile vendors have to provide
* vendor: vendor specific metrics (optional)
* application: application-specific metrics (optional)

NOTE: It is expected that a future version of this specification will also have a sub-resource for integrations
with other specifications of MicroProfile.

[[scopes]]
==== Scopes

===== Required Base metrics

Required base metrics describe a set of metrics that all MicroProfile-compliant servers have to provide.
Each vendor can implement the set-up of the metrics in the _base_ scope in a vendor-specific way.
The metrics can be hard coded into the server or read from a configuration file or supplied via the Java-API described in <<app-metrics-api>>.
The Appendix shows a possible data format for such a configuration.
The configuration and set up of the _base_ scope is thus an implementation detail and is not expected to be portable across vendors.

Section <<required-metrics>> lists the required metrics. This list also includes a few items marked as optional.
These are listed here as they are dependent on the underlying JVM and not the server and thus fit better in _base_ scope
than the _vendor_ one.

Required base metrics are exposed under `/metrics/base`.

===== Application metrics

Application specific metrics can not be baked into the server as they are supposed to be provided by the
application at runtime. Therefore a Java API is provided. Application specific metrics are supposed to be
portable to other implementations of the MicroProfile. That means that an application written to this
specification which exposes metrics, can expose the same metrics on a different compliant server
without change.

Details of this Java API are described in <<app-metrics-api>>.

Application specific metrics are exposed under `/metrics/application`.


===== Vendor specific Metrics

It is possible for MicroProfile server implementors to supply their specific metrics data on top
of the basic set of required metrics.
Vendor specific metrics are exposed under `/metrics/vendor`.

Examples for vendor specific data could be metrics like:

* OSGi statistics if the MicroProfile-enabled container internally runs on top of OSGi.
* Statistics of some internal caching modules

Vendor specific metrics are not supposed to be portable between different implementations
of MicroProfile servers, even if they are compliant with the same version of this specification.

[[metric_tags]]
==== Tags

Tags (or labels) play an important role in modern microservices and microservice scheduling systems (like e.g. Kubernetes).
Application code can run on any node and can be re-scheduled to a different node at any time. Each container in such
an environment gets its own ID; when the container is stopped and a new one started for the same image, it will get a
different id. The classical mapping of host/node and application runtime on it, therefore no longer works.

Tags have taken over the role to, for example, identify an application (`app=myShop`), the tier inside the application
(`tier=database` or `tier=app_server`) and also the node/container id. Metric value aggregation can then work over label
queries (Give me the API hit count for `app=myShop && tier=app_server`).

In MicroProfile Metrics, tags add an additional dimension to metrics that share a common basis. For example, a metric named
`carCount` can be further differentiated by the car type (sedan, SUV, coupe, and etc) and the colour (red, blue, white, black,
and etc). Rather than incorporating this in the metric name, tags can be used to capture this information in separate metrics.

[source]
----
carCount{type=sedan,colour=red}
carCount{type=sedan,colour=blue}
carCount{type=suv,colour=red}
carCount{type=coupe,colour=blue}
----

For portability reasons, the key name for the tag must match the regex `[a-zA-Z_][a-zA-Z0-9_]*` (Ascii alphabet, numbers and underscore).
If an illegal character is used, the implementation must throw an `IllegalArgumentException`.
If a duplicate tag is used, the last occurrence of the tag is used.

The tag value may contain any UTF-8 encoded character.

NOTE: The REST endpoints provided by MicroProfile Metrics have different reserved characters based on the format.
The characters are only escaped as needed when exposed through the REST endpoints.
See <<rest_endpoints>> for more information on the reserved characters.

Tags can be supplied in two ways:

* At the level of a metric as described in <<app-metrics-api>>.
* At the application server level by using https://github.com/eclipse/microprofile-config[MicroProfile Config] and
setting a configuration property of the name `mp.metrics.tags`. The implementation MUST make sure that an implementation of MicroProfile Config version at least 1.3 is available at runtime.
If it is supplied as an environment variable rather than system property, it can be named `MP_METRICS_TAGS` and will be picked up too.
** Tag values set through `mp.metrics.tags` MUST escape equal symbols `=` and commas `,` with a backslash `\`

.Set up global tags via environment variable
[source,bash]
----
export MP_METRICS_TAGS=app=shop,tier=integration,special=deli\=ver\,y
----

Global tags and tags registered with the metric are included in the output returned from the REST API.

NOTE: In application servers with multiple applications deployed, there is one reserved tag name: `_app`, which serves for
distinguishing metrics from different applications and must not be used for any other purpose. For details,
 see section <<app-servers>>.

[[meta-data-def]]
==== Metadata

Metadata can be specified for metrics in any scope. For base metrics, metadata must be provided by the implementation. Metadata is exposed by the REST handler.

TIP: While technically it is possible to expose metrics without (some) of the metadata, it helps tooling and also
operators when correct metadata is provided, as this helps getting a context and an explanation of the metric.

The Metadata:

* name: The name of the metric.
* unit: a fixed set of string units
* type:
** counter: a monotonically increasing numeric value (e.g. total number of requests received).
** concurrent gauge: an incrementally increasing or decreasing numeric value (e.g. number of parallel invocations of a method).
+
This type exposes three values: current count, highest count within the previous full minute and lowest count within the
previous full minute.
+
Full minute is the minute from second 0 to just before second 0 on the next minute ( eg. from [10:46:00-10:46:59.99999999] ).
** gauge: a metric that is sampled to obtain its value (e.g. cpu temperature or disk usage).
** meter: a metric which tracks mean throughput and one-, five-, and fifteen-minute exponentially-weighted moving average throughput.
** histogram: a metric which calculates the distribution of a value.
** timer: a metric which aggregates timing durations and provides duration statistics, plus throughput statistics.
* description (optional): A human readable description of the metric.
* displayName (optional): A human readable name of the metric for display purposes if the metric name is not
human readable. This could e.g. be the case when the metric name is a uuid.
* reusable (optional): If set to `true`, then it is allowed to register a metric multiple times under the same <<metricid-data-def>>.
Note that all such instances must set `reusable` to `true`.
Default is `true` for metrics created programmatically, `false` for metrics declared using annotations.
See <<reusing_metrics>> for more details.

Metadata must not change over the lifetime of a process (i.e. it is not allowed
to return the units as seconds in one retrieval and as hours in a subsequent one).
The reason behind it is that e.g. a monitoring agent on Kubernetes may read the
metadata once it sees the new container and store it. It may not periodically
re-query the process for the metadata.

IMPORTANT: In fact, metadata should not change during the life-time of the
whole container image or an application, as all containers spawned from it
will be "the same" and form part of an app, where it would be confusing in
an overall view if the same metric has different metadata.

=== Metric Registry
The `MetricRegistry` stores the metrics and metadata information.
There is one `MetricRegistry` instance for each of the scopes listed in <<scopes>>.

Metrics can be added to or retrieved from the registry either using the `@Metric` annotation
(see <<api-annotations, Metrics Annotations>>) or using the `MetricRegistry` object directly.

A metric is uniquely identified by the `MetricRegistry` if the `MetricID` associated with the metric is unique. That is to say, there are no other metrics with the same combination of metric name and tags. However, all metrics of the same name must be of the same type otherwise an `IllegalArgumentException` will be thrown. This exception will be thrown during registration.

The metadata information is registered under a unique metric name and is immutable. All metrics of the same name must be registered with the same metadata information otherwise an "IllegalArgumentException" will be thrown. This exception will be thrown during registration.

[[metricid-data-def]]
==== MetricID

The MetricID consists of the metric's name and tags (if supplied). This is used by the MetricRegistry to uniquely identify a metric and its corresponding metadata.

The MetricID:

* name: The name of the metric.
* tags (optional): A list of Tag objects. See also <<metric_tags>>.

[[reusing_metrics]]
==== Reusing Metrics

For metrics declared using annotations, by default it is not allowed to register more than one metric under a certain name and tags combination in a scope. This is done
to prevent hard to spot copy & paste errors, where for example all methods of a JAX-RS class are marked with
`@Timed(name="myApp", absolute=true)`.

If this behaviour is required, then it is possible to mark all such instances as _reusable_ by passing
the respective flag in the Annotation. Gauges are not reusable.

For metrics created programmatically (by calling methods of the `MetricRegistry`), reusing is allowed by default, so
multiple calls retrieving an instance of a metric from the registry will return the same metric object so that the object can be reused in
multiple places in the application.

The implementation must throw an 'IllegalArgumentException' during a metric registration call when the call would result
in the reuse of a metric where that metric was either previously declared not reusable or where the registration call itself
declares the metric to not be reusable.

Only metrics of the same type can be reused under the same MetricID.
Trying to reuse a name for different types will result in an `IllegalArgumentException`.
All metrics under the same name must also have exactly the same metadata.

TIP: If you want to re-use a MetricID, then you need to also explicitly set the `name` field OR set `absolute`
to `true` and have multiple methods annotated as metric that have the same method name and tags.

.Example of reused counters
[source,java]
----
    @Counted(name = "countMe", absolute = true, reusable = true, tags={"tag1=value1"})
    public void countMeA() { }

    @Counted(name = "countMe", absolute = true, reusable = true, tags={"tag1=value1"})
    public void countMeB() { }
----

In the above examples both `countMeA()` and `countMeB()` will share a single Counter with registered name `countMe` and the same tags in application scope.

[[cdi_scopes]]
==== Metrics and CDI scopes

Depending on CDI bean scope, there may be multiple instances of the CDI bean created over the lifecycle of an application.  Metrics, other than gauges, declared using annotations on CDI beans may therefore also have multiple instances. In these cases, where multiple metric instances exist corresponding to the instances of the CDI bean, updates to the value of the metric will be combined. For example, calls to a method annotated with `@Counted` will increase the value of the same counter no matter which bean instance is the one where the counted method is being invoked. Concurrent gauges will watch the number of parallel invocations of a method even if the invocations are on different instances.

The only exception from this are gauges (not concurrent gauges), which don't support multiple instances of the underlying bean to be created,
because in that case it would not be clear which instance should be used for obtaining the gauge value. For this reason,
gauges should only be used with beans that create only one instance, in CDI terms this means `@ApplicationScoped` and `@Singleton` beans. 
The implementation may employ validation checks that throw an error eagerly when it is detected that there is a `@Gauge` on a bean 
that will probably have multiple instances.
 

[[rest-api]]
=== Exposing metrics via REST API

Data is exposed via REST over HTTP under the `/metrics` base path in two different data formats for `GET` requests:

* JSON format - used when the HTTP Accept header best matches `application/json`.
* OpenMetrics text format - used when the HTTP Accept header best matches `text/plain` or when Accept header would equally
accept both `text/plain` and `application/json` and there is no other higher precedence format.
This format is also returned when no media type is requested (i.e. no Accept header is provided in the request)

NOTE: Implementations and/or future versions of this specification may allow for more export formats that are triggered
by their specific media type.
The OpenMetrics text format will stay as fall-back.

Formats are detailed below.

Data access must honour the HTTP response codes, especially

* 200 for successful retrieval of an object
* 204 when retrieving a subtree that would exist, but has no content. E.g. when the application-specific subtree has no application specific metrics defined.
* 404 if a directly-addressed item does not exist. This may be a non-existing sub-tree or non-existing object
* 406 if the HTTP Accept Header in the request cannot be handled by the server.
* 500 to indicate that a request failed due to "bad health". The body SHOULD contain details if possible { "details": <text> }

The API MUST NOT return a 500 Internal Server Error code to represent a non-existing resource.

.Supported REST endpoints
[cols="2,1,1,3"]
|===
| Endpoint | Request Type | Supported Formats | Description

| `/metrics` | GET | JSON, OpenMetrics | Returns all registered metrics
| `/metrics/<scope>` | GET | JSON, OpenMetrics | Returns metrics registered for the respective scope. Scopes are listed in <<metrics-setup>>
| `/metrics/<scope>/<metric_name>` | GET | JSON, OpenMetrics | Returns the metric that matches the metric name for the respective scope
| `/metrics` | OPTIONS | JSON | Returns all registered metrics' metadata
| `/metrics/<scope>` | OPTIONS | JSON | Returns metrics' metadata registered for the respective scope. Scopes are listed in <<metrics-setup>>
| `/metrics/<scope>/<metric_name>` | OPTIONS | JSON | Returns the metric's metadata that matches the metric name for the respective scope
|===

NOTE: The implementation must return a 406 response code if the request's HTTP Accept header for an OPTIONS request
does not match `application/json`.

[[app-servers]]
=== Usage of MicroProfile Metrics in application servers with multiple applications
Even though multi-app servers are generally outside the scope of MicroProfile, this section describes recommendations
how such application servers should behave if they want to support MicroProfile Metrics.

Metrics from all applications and scopes should be available under a single REST endpoint ending with `/metrics` similarly as
in case of single-application deployments (microservices).

To help distinguish between metrics pertaining to each deployed application,
a tag named `_app` should be appended to each metric. Its value should be equal to the context root of the web application to which the metric belongs.
For example, if a deployment is available under the `/cars` context root, each metric created by this deployment will contain an additional
tag named `_app` with a value of `/cars`. If the application server allows using metrics in JAR deployments, which have no web context,
the name of the JAR archive (including the `.jar` suffix) should be used. If such JAR is a module of an EAR application, the value of the `_app` tag should be
`ear_name#jar_name`.

This is an example JSON output from an application server that has applications under `/app1` and `/app2`, both of which have a counter metric
named `requestCount`:

----
{
  "requestCount;_app=/app1" : 198,
  "requestCount;_app=/app2" : 320
}
----

The value of the `_app` tag should be passed by the application server to the application via a MicroProfile Config property named `mp.metrics.appName`.
It should be possible to override this value by bundling the file `META-INF/microprofile-config.properties` within the application archive
and setting a custom value for the property `mp.metrics.appName` inside it.

It is allowed for application servers to choose to not add the `_app` tag at all, but in that case, metrics from two applications on
one server can clash as no differentiator (by application) is given.

There should be a single `MetricRegistry` instance shared between all applications to prevent unexpected clashes when merging the contents
of different registries while responding to metric export requests. It is up to the application server whether it will allow sharing
of metrics between different applications (for example, if there's a reusable metric in one application, another might want to reuse it).

==== Implementation notes:
Constructors of the `MetricID` class from the API code already handle adding the `_app` tag automatically
when they detect that there is a property named `mp.metrics.appName` available from the `org.eclipse.microprofile.config.Config` instance
available in the current context. If no such property exists or if the value is empty, no tag will be appended.

Generally, the responsibility of the application server implementation will be to append a property `mp.metrics.appName` to the
`org.eclipse.microprofile.config.Config` instance of each application during deployment time, its value being the web context root of the application
or the JAR name. This can be achieved for example by adding a custom `ConfigSource` with an ordinal less than 100, because
the `ConfigSource` that reads properties `META-INF/microprofile-config.properties` has an ordinal of 100, and this needs to have higher priority.
